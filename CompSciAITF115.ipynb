{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pO4-CY_TCZZS"
   },
   "source": [
    "# Train a Simple Audio Recognition model for microcontroller use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BaFfr7DHRmGF"
   },
   "source": [
    "This notebook demonstrates how to train an approximately 20kb [Simple Audio Recognition](https://www.tensorflow.org/tutorials/sequences/audio_recognition) model for [TensorFlow Lite for Microcontrollers](https://tensorflow.org/lite/microcontrollers/overview). It will produce a model that can be used in the [micro_speech](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/micro_speech) example application.\n",
    "\n",
    "The notebook has been adapted from the [example](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/micro_speech/train_speech_model.ipynb) shipped with TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XaVtYN4nlCft"
   },
   "source": [
    "The notebook runs Python scripts to train and freeze the model, and uses the TensorFlow Lite converter to convert it for use with TensorFlow Lite for Microcontrollers.\n",
    "\n",
    "**Training is much faster using GPU acceleration.** Before you proceed, ensure you are using a GPU runtime by going to **Runtime -> Change runtime type** and selecting **GPU**. Training 18,000 iterations will take 1.5-2 hours on a GPU runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that tensorflow is the right version, 1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure training\n",
    "\n",
    "The following `os.environ` lines can be customized to set the words that will be trained for, and the steps and learning rate of the training. The default values will result in the same model that is used in the micro_speech example. Run the cell to set the configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ludfxbNIaegy"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# A comma-delimited list of the words you want to train for.\n",
    "# The options are: yes,no,up,down,left,right,on,off,stop,go\n",
    "# All other words will be used to train an \"unknown\" category.\n",
    "os.environ[\"WANTED_WORDS\"] = \"stop\"\n",
    "\n",
    "# The number of steps and learning rates can be specified as comma-separated\n",
    "# lists to define the rate at each stage. For example,\n",
    "# TRAINING_STEPS=15000,3000 and LEARNING_RATE=0.001,0.0001\n",
    "# will run 18,000 training loops in total, with a rate of 0.001 for the first\n",
    "# 15,000, and 0.0001 for the final 3,000.\n",
    "os.environ[\"TRAINING_STEPS\"]=\"15000,3000\"\n",
    "os.environ[\"LEARNING_RATE\"]=\"0.001,0.0001\"\n",
    "\n",
    "# Calculate the total number of steps, which is used to identify the checkpoint\n",
    "# file name.\n",
    "total_steps = sum(map(lambda string: int(string),\n",
    "                  os.environ[\"TRAINING_STEPS\"].split(\",\")))\n",
    "os.environ[\"TOTAL_STEPS\"] = str(total_steps)\n",
    "\n",
    "# Print the configuration to confirm it\n",
    "!echo \"Training these words: ${WANTED_WORDS}\"\n",
    "!echo \"Training steps in each stage: ${TRAINING_STEPS}\"\n",
    "!echo \"Learning rate in each stage: ${LEARNING_RATE}\"\n",
    "!echo \"Total number of training steps: ${TOTAL_STEPS}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gCgeOpvY9pAi"
   },
   "source": [
    "## Dependencies\n",
    "\n",
    "Make sure that `xxd` is installed\n",
    "```\n",
    "sudo apt install xxd\n",
    "```\n",
    "\n",
    "Assume that we set up the Anaconda environment for Tensorflow 1.15 with GPU support\n",
    "\n",
    "```\n",
    "conda create -n tf15-gpu tensorflow-gpu=1.15\n",
    "conda activate tg15-gpu\n",
    "conda install anaconda-navigator\n",
    "```\n",
    "\n",
    "We will want tensorflow in the same directory as this notebook, which could be `${HOME}/notebooks`\n",
    "```\n",
    "cd ${HOME}/notebooks\n",
    "git clone -q https://github.com/tensorflow/tensorflow\n",
    "```\n",
    "The `tensorflow` repo contains the scripts that train and freeze the model.\n",
    "\n",
    "Check out a commit that has been tested to work with the build of TensorFlow we are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "APGx0fEh7hFF"
   },
   "outputs": [],
   "source": [
    "!git -c advice.detachedHead=false -C tensorflow checkout 17ce384df70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, make sure there is a `content` directory for the work below.\n",
    "```\n",
    "cd ${HOME}/notebooks\n",
    "mkdir content\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aV_0qkYh98LD"
   },
   "source": [
    "## Load TensorBoard\n",
    "\n",
    "Now, set up TensorBoard so that we can graph our accuracy and loss as training proceeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yZArmzT85SLq"
   },
   "outputs": [],
   "source": [
    "# Delete any old logs from previous runs\n",
    "!rm -rf ./content/retrain_logs\n",
    "# Load TensorBoard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./content/retrain_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x1J96Ron-O4R"
   },
   "source": [
    "## Begin training\n",
    "\n",
    "Next, run the following script to begin training. The script will check for, and if needed download, the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VJsEZx6lynbY"
   },
   "outputs": [],
   "source": [
    "!python tensorflow/tensorflow/examples/speech_commands/train.py \\\n",
    "--model_architecture=tiny_conv --window_stride=20 --preprocess=micro \\\n",
    "--wanted_words=${WANTED_WORDS} --silence_percentage=25 --unknown_percentage=25 \\\n",
    "--quantize=1 --verbosity=WARN --how_many_training_steps=${TRAINING_STEPS} \\\n",
    "--learning_rate=${LEARNING_RATE} --summaries_dir=./content/retrain_logs \\\n",
    "--data_dir=./content/speech_dataset --train_dir=./content/speech_commands_train \\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XQUJLrdS-ftl"
   },
   "source": [
    "## Freeze the graph\n",
    "\n",
    "Once training is complete, run the following cell to freeze the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xyc3_eLh9sAg"
   },
   "outputs": [],
   "source": [
    "!python tensorflow/tensorflow/examples/speech_commands/freeze.py \\\n",
    "--model_architecture=tiny_conv --window_stride=20 --preprocess=micro \\\n",
    "--wanted_words=${WANTED_WORDS} --quantize=1 --output_file=./content/tiny_conv.pb \\\n",
    "--start_checkpoint=./content/speech_commands_train/tiny_conv.ckpt-${TOTAL_STEPS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_DBGDxVI-nKG"
   },
   "source": [
    "## Convert the model\n",
    "\n",
    "Run this cell to use the TensorFlow Lite converter to convert the frozen graph into the TensorFlow Lite format, fully quantized for use with embedded devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lBj_AyCh1cC0"
   },
   "outputs": [],
   "source": [
    "!toco \\\n",
    "--graph_def_file=./content/tiny_conv.pb --output_file=./content/tiny_conv.tflite \\\n",
    "--input_shapes=1,49,40,1 --input_arrays=Reshape_2 --output_arrays='labels_softmax' \\\n",
    "--inference_type=QUANTIZED_UINT8 --mean_values=0 --std_dev_values=9.8077"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dt6Zqbxu-wIi"
   },
   "source": [
    "The following cell will print the model size, which will be under 20 kilobytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XohZOTjR8ZyE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "model_size = os.path.getsize(\"./content/tiny_conv.tflite\")\n",
    "print(\"Model is %d bytes\" % model_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2pQnN0i_-0L2"
   },
   "source": [
    "Finally, we use xxd to transform the model into a source file that can be included in a C++ project and loaded by TensorFlow Lite for Microcontrollers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eoYyh0VU8pca"
   },
   "outputs": [],
   "source": [
    "# Save the file as a C source file\n",
    "!xxd -i ./content/tiny_conv.tflite > ./content/tiny_conv.cc\n",
    "# Print the source file\n",
    "!cat ./content/tiny_conv.cc"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Train simple audio recognition model",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
